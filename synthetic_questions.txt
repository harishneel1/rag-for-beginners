Basic Architecture Questions

1. What are the two main components of the Transformer architecture? 
2. How many layers does the base Transformer model use in both encoder and decoder? 
3. What is the dimensionality (dmodel) used in the base Transformer model? 

Attention Mechanism Questions

4. What is the formula for Scaled Dot-Product Attention? 
5. Why do the authors scale the dot products by 1/âˆšdk in their attention mechanism? 
6. How many attention heads does the Transformer use, and what is the dimension of each head? 

Comparative Analysis Question

7. According to Table 1, what are the main advantages of self-attention layers compared to recurrent and convolutional layers in terms of computational complexity and parallelization? 